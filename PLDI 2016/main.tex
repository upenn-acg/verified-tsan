\documentclass[preprint, 10pt]{sigplanconf}

\usepackage{uri}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{semantic}
\usepackage{graphicx}
\usepackage{cases}
\usepackage{xcolor}
\usepackage{xspace}

\newcommand{\TODO}[1]{\textbf{\textcolor{red}{[ TODO: #1]}}}
\newcommand{\ignore}[1]{}
\newcommand{\con}[1]{\ensuremath{\mathsf{consistent}(#1)}}
\newcommand{\seqcon}[1]{\ensuremath{\mathsf{seq\_con}(#1)}}
\newcommand{\mread}[2]{\ensuremath{\mathsf{read}(#1, #2)}}
\newcommand{\mwrite}[2]{\ensuremath{\mathsf{write}(#1, #2)}}
\newcommand{\malloc}[2]{\ensuremath{\mathsf{alloc}(#1, #2)}}
\newcommand{\mfree}[1]{\ensuremath{\mathsf{free}(#1)}}
%\newcommand{\cccando}[2]{\ensuremath{\mathsf{can\_do_{CC}}(#1, #2)}}
\newcommand{\hb}[0]{<_{\mathrm{hb}}}
\newcommand{\po}[0]{<_{\mathrm{po}}}
\newcommand{\sw}[0]{<_{\mathrm{sw}}}
\newcommand{\word}[0]{<_{\mathrm{w}}}

\newcommand{\assign}[2]{#1\ \texttt{:=}\ #2}
\newcommand{\load}[2]{#1\ \texttt{:= load}\ #2}
\newcommand{\store}[2]{\texttt{store}\ #2\ #1}
\newcommand{\lock}[1]{\texttt{lock}\ #1}
\newcommand{\unlock}[1]{\texttt{unlock}\ #1}
\newcommand{\spawn}[2]{\texttt{spawn}\ #1\ #2}
\newcommand{\wait}[1]{\texttt{wait}\ #1}
\newcommand{\assert}[2]{\texttt{assert(}#1\ \texttt{<=}\ #2\texttt{)}}
\newcommand{\move}[2]{\ensuremath{\mathsf{move}(#1, #2)}}
\newcommand{\setvc}[2]{\ensuremath{\mathsf{set}(#1, #2)}}
\newcommand{\incvc}[2]{\ensuremath{\mathsf{inc}(#1, #2)}}
\newcommand{\maxa}[2]{\ensuremath{\mathsf{max}(#1, #2)}}
\newcommand{\maxvc}[2]{\ensuremath{\mathsf{merge}(#1, #2)}}
\newcommand{\lea}[2]{\ensuremath{\mathsf{lea}(#1, #2)}}
\newcommand{\vcle}[2]{\ensuremath{\mathsf{hb\_check}(#1, #2)}}
\newcommand{\instr}[2]{\ensuremath{\mathsf{instrument}(#1, #2)}}

%from StackExchange, a better xrightarrow^*
\newcommand{\tto}[1]{\mathrel{
  \vphantom{\xrightarrow{#1}}
  \smash{\xrightarrow{#1}}
  \vphantom{\to}^{*}}
}
\newcommand{\tTo}[1]{\mathrel{
  \vphantom{\xrightarrow{#1}}
  \smash{\xRightarrow{#1}}
  \vphantom{\to}^{*}}
}

\hyphenation{Comp-Cert}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{principle}{Principle}


\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{PLDI '16}{June 13--17, 2016, Santa Barbara, California, United States} 
\copyrightyear{2016}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
%\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Verifying Dynamic Race Detection}
\ignore{\authorinfo{William Mansky \and Yuanfeng Peng \and Steve Zdancewic \and Joseph Devietti}
           {University of Pennsylvania}
           {wmansky@seas.upenn.edu, yuanfeng@cis.upenn.edu, stevez@cis.upenn.edu, devietti@cis.upenn.edu}}
\authorinfo{}{}{}
\maketitle

\begin{abstract}
Writing race-free concurrent code is notoriously difficult, and races can result in bugs that are difficult to isolate and reproduce. Dynamic race detection is often used to catch races that cannot (easily) be detected statically. One approach to dynamic race detection is to instrument the potentially racy code with operations that store and compare metadata, where the metadata implements some known race detection algorithm (e.g. vector clock race detection). In this paper, we lay out an instrumentation pass for race detection in a simple language, and present a mechanized formal proof of its correctness: all races in a program will be caught by the instrumentation, and all races detected by the instrumentation are possible in the original program.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

\keywords
dynamic race detection, interactive theorem proving

\section{Introduction}

Multicore processors have steadily invaded a broad swathe of the computing ecosystem in everything from datacenters to smart watches. Writing multithreaded code for such platforms can bring good performance but also a host of programmability challenges. One of the key challenges is dealing with data races, as the presence of a data race introduces non-sequentially-consistent \cite{manson_java_2005} and in some cases undefined \cite{boehm_foundations_2008} behavior into programs, making program semantics very difficult to understand. There are many systems that provide sound and complete data race detection via dynamic analysis \cite{djit+,fasttrack,slimstate} to help programmers detect and remove data races from their programs. 

While these analyses have been proven correct, such proofs have two main shortcomings: they are proofs of the algorithms, instead of the implementations, and they are paper proofs instead of machine-checked proofs. Because of these shortcomings, it is possible that a race detector does not faithfully implement its algorithm, or that the algorithm itself is not fully correct. Indeed, our verified approach reveals a small issue in the paper proof from \cite{fasttrack} that, while fixable, illustrates the potential dangers that stem from best-effort proofs and implementations.

Our approach seeks to rectify these concerns and place dynamic race detection on a provably-correct foundation by providing the first fully-verified implementation of vector clock race detection. We adopt a two-level approach, first verifying the high-level algorithm and then verifying that an instrumentation pass for a simple language faithfully implements the algorithm and preserves the semantics of the original, uninstrumented program. We believe that our approach is useful for verifying race detection algorithms and implementations, which we show by verifying the correctness of the FastTrack \cite{fasttrack} algorithm. Crucially, our verification of FastTrack is achieved by proving it equivalent to the simpler vector clock race detection algorithm, which is more efficient than proving FastTrack's correctness directly. To the best of our knowledge, ours is the first scheme to adopt formal verification for either race detection algorithms \emph{or} their implementation. Moreover, we believe our general approach may be useful for verifying a broad range of dynamic analyses, helping to ensure that debugging tools are themselves free from bugs.

\TODO{need more technical nuggets here, and/or a figure?}

This paper makes the following contributions:

\begin{itemize}
\item We give the first mechanized proofs of correctness of vector clock race detection and the FastTrack algorithm
\item We discovered an issue in the paper proof of correctness for FastTrack that we repair in our mechanized proof
\item We describe an instrumentation pass that has been proved to implement vector clock race detection for a simple language
\item We present a method for verifying race detection instrumentation (the deep spec approach)
\end{itemize}

\section{Proof Strategy}
Ultimately, our goal for each algorithm and program transformation presented in this paper is to prove that it implements sound and complete race detection, i.e., that it raises an alarm in all racy executions and only racy executions. However, as much as possible, we prefer not to do this by referring back to the base definition of racy executions. We begin by stating and proving correctness of a simple vector clock race detection algorithm, by direct relation to the definition of a race. We then prove further results---the correctness of a more sophisticated algorithm, and of an instrumentation pass meant to implement the simple algorithm---by relating them to the verified base algorithm. We may think of this hierarchy in terms of specifications and refinement: we begin by proving that the base algorithm refines the abstract specification of race detection, and then use it in turn as a specification refined by more complex or detailed mechanisms. (diagram?) This allows us to separate concerns and avoid duplicating proof effort, but it also serves as further validation of the base vector clock algorithm: by showing that it is \emph{two-sided}, that is, that it both implements a higher-level specification of its desired behavior and is implemented by more concrete systems, we gain confidence that it is correctly stated (and not, e.g., vacuously correct). \TODO{more detail on the approach to instrumentation, of program + abstract algorithm?}

\section{Race Detection Algorithms}
\subsection{Defining Races}


\subsection{Vector Clock Race Detection}
definition

\subsubsection{Correctness}
The correctness of vector clock race detection follows from the fact that the $\sqsubseteq$ relation between vector clocks precisely models the happens-before relation. Our formalization follows the proof outline for FastTrack~\cite{fasttrack}. \TODO{Should we mention the bug here or in the FastTrack section?}

\subsection{FastTrack}
definition

\subsubsection{Correctness}
We prove correctness of this algorithm by relating it to the base vector clock algorithm. Intuitively, the metadata optimizations of FastTrack are safe to perform because the reduced metadata still captures the same happens-before relationships, which means that the same $\sqsubseteq$ relationships hold between corresponding state components. Thus, we need only present a relation between FastTrack states and full vector clock states that captures this correspondence in order to prove a bisimulation between the two systems. The $C$ and $L$ components should remain unchanged. To characterize the expected semantics of epochs, we define the following \emph{representation} relation:
\begin{definition}An epoch $e$ \emph{represents} a vector clock $V$ to another vector clock $V'$ if $e \preceq V'$ implies that $V \sqsubseteq V'$. An epoch $e$ represents $V$ in a state $(C, L, R, W)$ if it represents $V$ to $C_u$ for all $u$ and $L_m$ for all $m$.\end{definition}

In FastTrack, when write metadata $W_x$ is collapsed to an epoch $c@t$, it is precisely because $W_x(t) = c$ and $t$ is representative of $W_x$. The relation for read metadata is more complicated, for one particular reason: the Write Shared rule resets read metadata to an empty epoch. This has two effects on the simulation relation. First, we must make a special allowance for the case in which the read metadata is empty; in this case, it is the write epoch that represents the original read vector clock. Second, even when the FastTrack state holds a vector clock in $R_x$, that vector clock may contain some 0 values in places where reads have in fact been performed. In effect, because FastTrack read vector clocks are always derived from epochs, they carry forward a partial representation relation.
\begin{definition}Suppose we have a pair of vector clock and FastTrack states $(C, L, R, W)$, $(C', L', R', W')$. A thread $t$ \emph{partially represents} $R_x$ to another vector clock $V'$ if for all $t'$ such that $R'_x(t') = 0$, $R_x(t) \le V'(t)$ implies that $R_x(t') \le V'(t')$, and for all $t'$ such that $R'_x(t') \neq 0$, $R'_x(t') \le V'(t')$ implies that $R_x(t') \le V'(t')$. A thread $t$ partially represents $R_x$ if it partially represents $R_x$ to $C_u$ for all $u$ and $L_m$ for all $m$.\end{definition}

We can now state the full simulation relation between vector clock and FastTrack states.
\begin{definition}A vector clock state $(C, L, R, W)$ and a FastTrack state $(C', L', R', W')$ are in the relation $\sim$ when $C' = C$, $L' = L$, and for every location $x$:
\begin{itemize}
\item if $W'_x = c@t$, then $W_x(t) = c$ and $c@t$ represents $W_x$
\item $R'_x(t) \le R_x(t)$ for all $t$
\item if $R'_x = \bot_e$, then $W'_x$ represents $R_x$
\item if $R'_x = c@t$, then $R_x(t) = c$ and $c@t$ represents $R_x$
\item if $R'_x = V$, then there exists a thread $t$ such that $t$ partially represents $R_x$
\end{itemize}
\end{definition}
\begin{theorem}The relation $\sim$ is a bisimulation.\end{theorem}

\subsubsection{Alternative Proof}
Flanagan and Freund justify the optimizations of FastTrack by proving that the $\sqsubseteq$ relation on vector clocks still precisely captures the happens-before relation. We formalized this proof in Coq, and in the process, discovered an error in the paper proof. FastTrack's Lemma 4 states the following: {\it Suppose $\sigma$ is well-formed and $\sigma \Rightarrow^\alpha \sigma'$ and $a, b \in \alpha$. Let $t = \mathit{tid}(a)$ and $u = \mathit{tid}(b)$. If $a <_{\alpha} b$ then $K^a(t) \sqsubseteq K^b(t)$.} This lemma is then used to prove that if some operation $b$ is \emph{stuck}, then a previous operation $a$ must have raced with it, since $K^a(t) \not\sqsubseteq K^b(t)$. However, the premise of Lemma 4 requires that $a$ and $b$ not be stuck, sincen they are in a trace $\alpha$ that successfully executes to a state $\sigma'$. Because of this constraint, Lemma 4 in fact does not apply. Fortunately, this premise is stronger than necessary to prove Lemma 4, and in Coq we are able to prove a more general version:
\begin{lemma}Suppose $\sigma$ is well-formed and $\sigma \Rightarrow^\alpha \sigma'$ and $a \in \alpha$. Let $t = \mathit{tid}(a)$ and $u = \mathit{tid}(b)$. If $a <_{\alpha; b} b$ then $K^a(t) \sqsubseteq K^b(u)$.\end{lemma}
where by abuse of notation we use $K^b(t)$ to refer to the $C_u$ component of $\sigma'$ updated as appropriate for a blocking operation. \TODO{Proof?} With this statement of the lemma, the completeness proof follows as outlined in the paper. This proof is about 130 lines longer than the proof by simulation \TODO{that's not that much}, reflecting duplication of effort from proving very similar theorems about the two systems.

\section{Instrumenting a Simple Language}
\subsection{The Language}
We define a simple multithreaded language that is just complicated enough to have races and implement race detection instrumentation. The instructions of the language are defined as follows, where $n$ is a natural number, $a$ is a local variable, $e, e1, e2$ are expressions, $p$ is a pointer, $l$ is a lock, $t$ is a thread id, and $i_j$ are instructions:
\begin{align*}\mathit{expr} ::= n~|~a~|~e_1 + e_2~|~\mathtt{max}(e_1, e_2)\end{align*}
\begin{align*}\mathit{instr} ::=\ &\assign{a}{e}~|~\load{a}{p}~|~\store{p}{e}~|~\lock{l}~\\|~&\unlock{l}~|~\spawn{t}{i_1, ..., i_n}~|~\wait{t}~\\|~&\assert{e_1}{e_2}\end{align*}
A \emph{program} is simply a list of instructions. The dynamic state of a program contains a collection of threads, where a thread is a pair $(t, \mathit{li})$ of a tid and a list of instructions, and a family of local environments $G_t$, one for each thread, assigning values to local variables. We give semantics to the language via a transition system of the form $(P, G) \xrightarrow{o, c} (P', G')$, where $P, P'$ are collections of threads, $G, G'$ are environments, $o$ is a race detection operation (if one is produced by the step), and $c$ is a concurrent memory operation (likewise). We define a function $\mathsf{eval}$ that takes a local environment and an expression and yields the value of the expression. The semantics of the language are shown in Figure~\ref{semantics}. \TODO{I'm not satisfied with this presentation; can we distinguish memory and RD ops better?}

\begin{figure}[htb]
$$\inference[assign]{P(t) = \assign{a}{e}\texttt{;} \mathit{li}}{(P, G) \rightarrow_t \\(P[t \mapsto \mathit{li}], G[G_t[a \mapsto \mathsf{eval}(G_t, e)]])}$$

\caption{Semantics of the simple language}
\label{semantics}
\end{figure}


\subsection{Instrumentation}
For each rule of the vector clock algorithm, we provide a code snippet that can be added to the associated instruction to implement the rule. We begin by defining macros for the basic mathematical operations of the algorithm, as shown in Figure~\ref{helper}. We designate two local variables unused in the base program (\texttt{tmp1} and \texttt{tmp2}) as temporaries for use by the instrumentation. Let $z$ be the number of threads generated over the lifetime of the program. (Note that in our simple language, $z$ can be determined statically; in real-world implementations, the vector clock data would need to be dynamically resized in memory if more threads were created.)

\begin{figure}[htb]
\move{p}{q} $\triangleq$ \load{\texttt{tmp1}}{$p$}\texttt{;} \store{$q$}{\texttt{tmp1}}

\setvc{a}{b} $\triangleq$ \move{(a, 0)}{(b, 0)}\texttt{;} ...\texttt{;} \move{(a, z - 1)}{(b, z - 1)}
$(V = V')$

\incvc{a}{b} $\triangleq$ \load{\texttt{tmp1}}{$(a, t)$}\texttt{;} \assign{\texttt{tmp1}}{\texttt{tmp1 + 1}}\texttt{;} \store{$(a, t)$}{\texttt{tmp1}}
$(C = C[t := \mathit{inc}_t(C_t)])$

\maxa{p}{q} $\triangleq$ \load{\texttt{tmp1}}{$p$}\texttt{;} \load{\texttt{tmp2}}{$q$}\texttt{;} \assign{\texttt{tmp2}}{\texttt{max\ tmp1\ tmp2}}\texttt{;} \store{$p$}{\texttt{tmp2}}

\maxvc{a}{b} $\triangleq$ \maxa{(a, 0)}{(b, 0)}\texttt{;} ...\texttt{;} \maxa{(a, z - 1)}{(b, z - 1)}
$(C' = C \sqcup C')$

\lea{p}{q} $\triangleq$ \load{\texttt{tmp1}}{$p$}\texttt{;} \load{\texttt{tmp2}}{$q$}\texttt{;} \assert{\texttt{tmp1}}{\texttt{tmp2}}

\vcle{a}{b} $\triangleq$ \lea{(a, 0)}{(b, 0)}\texttt{;} ...\texttt{;} \lea{(a, z - 1)}{(b, z - 1)}
$(C \sqsubseteq C')$
\caption{Helper macros}
\label{helper}
\end{figure}

With these macros as building blocks, we can straightforwardly translate the rules of vector clock race detection (cite figure) into code. We set aside dedicated areas of memory for each of the components of the vector clock state, labeled $C$, $L$, $R$, and $W$ accordingly, and index into them by assigning a unique numeric identifier to each thread, local variable, and lock, so that the block at $L + m$ in memory contains the vector clock $L_m$. Note that the timing of the associated vector clock operations depends on the instruction being instrumented; in particular, the blocking operations \texttt{lock} and \texttt{wait} must not change the vector clock state until after they successfully complete.

\begin{figure}[htb]
\instr{t}{\load{a}{(b, o)}} $\triangleq$ \vcle{W + b}{C + t}\texttt{;} \move{(C + t, t)}{(R + b, t)}\texttt{;} \load{$a$}{$(b, o)$}

\instr{t}{\store{(b, o)}{e}} $\triangleq$ \vcle{W + b}{C + t}\texttt{;} \vcle{R + b}{C + t}\texttt{;} \move{(C + t, t)}{(W + b, t)}\texttt{;} \store{$(b, o)$}{$e$}

\instr{t}{\lock{m}} $\triangleq$ \lock{$m$}\texttt{;} \maxvc{L + m}{C + t}

\instr{t}{\unlock{m}} $\triangleq$ \setvc{C + t}{L + m}\texttt{;} \incvc{t}{C + t}\texttt{;} \unlock{$m$}

\instr{t}{\spawn{u}{\mathit{li}}} $\triangleq$ \maxvc{C + t}{C + u}\texttt{;} \incvc{t}{C + t}\texttt{;} \spawn{$u$}{$\mathit{li}$}

\instr{t}{\wait{u}} $\triangleq$ \wait{$u$}\texttt{;} \maxvc{C + u}{C + t}\texttt{;} \incvc{u}{C + u}
\caption{Instrumentation}
\label{instrumentation}
\end{figure}

\subsection{Necessary Synchronization}
The instrumentation of the previous section cannot implement sound and complete race detection for one important reason: when a race does occur, the corresponding instrumentation also races. \TODO{example} Poorly synchronized race detection instrumentation cannot hope to successfully detect all races. At the same time, adding too much synchronization could significantly hurt performance. It suffices to add a lock for each memory location, which is used to protect the instrumentation on that memory location. To this end, we add another area of memory, $X$, where $X + b$ holds the lock for block $b$, and add locking to the \texttt{load} and \texttt{store} instrumentation:
$$\instr{t}{\load{a}{(b, o)}} \triangleq \lock{X + x}\texttt{;} \vcle{W + b}{C + t}\texttt{;} \move{(C + t, t)}{(R + b, t)}\texttt{;} \load{a}{(b, o)}\texttt{;} \unlock{X + x}$$
$$\instr{t}{\store{(b, o)}{e}} \triangleq \lock{X + x}\texttt{;} \vcle{W + b}{C + t}\texttt{;} \vcle{R + b}{C + t}\texttt{;} \move{(C + t, t)}{(W + b, t)}\texttt{;} \store{(b, o)}{e}\texttt{;} \unlock{X + x}$$

\label{waits}
The semantics of our language allow one more case in which instrumentation may race. Since a terminated thread is not removed from the state when another thread \texttt{wait}s for it, two threads may \texttt{wait} for the same thread, and the subsequent race detection operations may conflict. While this is a fairly obscure case, on closer examination, it is also preventable. The $\mathsf{merge}$ operation only reads the vector clock of the terminated thread, and simultaneous reads to the same location do not constitute races. The only write to $C + u$ is in the $\mathsf{inc}$ macro, which is performed to maintain the invariant that a thread always has a higher timestamp for itself than any other thread has for it. While this simplifies reasoning, the invariant is not actually necessary for a thread that will perform no further operations, which must be the case for the target of a \texttt{wait}. If we remove the $\mathsf{inc}$ from the \texttt{wait} instrumentation, we get an equally correct race detection algorithm that allows for multiple \texttt{waits} on a single thread \TODO{proof?}.

\section{Verifying Instrumentation}
We verify the correctness of the instrumentation by showing that instrumentation records the same information and performs the same checks as the vector clock algorithm would perform on the input program.

Key to our correctness proof is the idea that the instrumented program can be thought of as executing under a bigger-step semantics in which each instruction and its instrumentation execute in a single step. This semantics is shown in Figure~\TODO{?}
\begin{lemma}\label{iexec-exec}If $(P, G) \xRightarrow{\vec{a}, \vec{b}}_t (P', G')$, then $(P, G) \xrightarrow{\vec{a}, \vec{b}}_t (P', G')$.\end{lemma}
\begin{proof}By case analysis and application of the relevant small-step rules.\end{proof}

We would also like to prove the correspondence in the other direction, but this is much more difficult. Any given execution of an instrumented program may not line up with one in which the instrumentation for each instruction executes in a single step; at a state in the middle of the execution, the program may be in the process of executing as many different instrumentation sections as there are threads. We resolve this problem by showing that we can ``reorder'' the steps of any execution so that the instrumentation for each instruction executes contiguously. More precisely, we show that each execution of an instrumented program is equivalent to one in which the instrumentation executes atomically.

The reordering proceeds inductively: given an execution, we can always find the first complete instrumentation section, move its steps to the front of the execution, and then continue on the remaining steps. We begin by defining the uninstrumented state represented by each intermediate state of the execution.
\begin{definition}An instrumented state $P$ is an \emph{instrumented suffix} of a state $P_0$ if $P_0$ and $P$ contain the same threads and for each thread $t$, $P(t)$ is obtained by dropping some prefix of the instrumentation of the first instruction from $\instr{t}{P_0(t)}$ (or is empty if $P_0(t)$ is empty).\end{definition}

Steps by a thread have no effect on the state or local environment of other threads. The only way in which they communicate is via their effects on the shared memory. As such, the main obligation in proving that we can reorder steps in an execution is to show that reordering the associated memory operations does not change the behavior of the program. For our purposes, it suffices to show the stronger condition that if two instrumentation sections execute simultaneously, then the memory locations that they access do not overlap. We refer to this property as \emph{noninterference}. In the following, we use $P \rightarrow^{*}_t P'$ to mean that there is a (possibly empty) sequence of steps $P \rightarrow_t P_1 \rightarrow_t ... \rightarrow_t P'$, and $P \rightarrow^{*}_{\not t} P'$ to mean that there is a (possibly empty) sequence of steps $P \rightarrow_a P_1 \rightarrow_b ... \rightarrow_z P'$ where $a, b, ..., z \neq t$.

\begin{lemma}\label{indep}Let $P_0$ be a well-formed uninstrumented program and $P_0'$ its instrumented counterpart. Suppose we have some state $P'$ and instruction $i$ such that $P_0' \rightarrow^{*} P'$ and $P'(t) = \instr{t}{i}; \mathit{li}$, and $P' \xrightarrow{o1, c1}_t P_1 \tto{\vec{a}, \vec{b}} P_2 \xrightarrow{o2, c2}_u P_3$ such that $P_2(t) = i'; ...; \mathit{li}$. Then the operations performed by $t$ in $c1; \vec{b}; c2$ do not overlap with the operations performed by threads other than $t$ in $c1; \vec{b}; c2$.\end{lemma}
\begin{proof}By case analysis on the instruction $i$. In the cases of assignment and \texttt{assert} statements, no memory operations are produced. For \texttt{load} and \texttt{store} instructions on a location $(b, o)$, the lock $X + b$ protects all associated memory locations, and so no other thread can access them until the lock is released. For \texttt{lock} and \texttt{unlock} instructions on a lock $m$, $m$ itself protects its associated metadata, and likewise no other thread can access until the instrumentation is finished and the lock released. For \texttt{spawn} instructions, the thread is not spawned until the end of the instrumentation, and so no other thread can interact with it or its metadata. Likewise, for \texttt{wait} instructions, the instrumentation cannot begin to execute until the target of the \texttt{wait} has terminated, and at that point no other thread will access its metadata (we assume that only one thread \texttt{wait}s for each thread; see the note in Section~\ref{waits}).\end{proof}

This lemma lets us gather up the steps by a thread $t$ that make up an instrumentation section, and reorder them into a consecutive sequence.
\begin{lemma}[Noninterference]\label{noninterference}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. Suppose that $P \tto{\vec{a}, \vec{b}}_t P_1 \tto{\vec{c}, \vec{d}}_{\not t} P_2 \xrightarrow{o, c}_t P_3$ such that $P_2$ is an instrumented suffix of $P_0$. Then $c$ does not overlap with the operations in $\vec{d}$.\end{lemma}
\begin{proof}By induction on the derivation of $P_1 \tto{\vec{c}, \vec{d}}_{\not t} P_2$. Since $P_2$ is an instrumented suffix of $P_0$, we know that no instrumentation section is completed in this section of the execution. Thus, each step by a thread $u \neq t$ is either the first step of an instrumentation section, or somewhere in the middle of an instrumentation section. In the former case, we know from Lemma~\ref{indep} that the following operations in $\vec{d}; c$ by threads other than $u$ do not overlap with the operations by $u$. In the latter case, there must have previously existed a first step by $u$ that began executing the instrumentation section, and again we know from Lemma~\ref{indep} that all operations in $\vec{d}; c$ by threads other than $u$ do not overlap with operations by $u$. We can conclude that the operations by each thread in $\vec{d}; c$ are to disjoint locations. Since none of the operations in $\vec{d}$ are by $t$, this means that $c$ does not overlap with any of the operations in $\vec{d}$.\end{proof}
\begin{lemma}\label{first-finished}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. Suppose that $P \tto{\vec{a}, \vec{b}} P_1 \xrightarrow{o, c}_t P_2$, where $P_1$ is an instrumented suffix of $P_0$ and the step from $P_1$ to $P_2$ completes an instrumentation section. Then there exists a state $P'$ such that $P \xRightarrow{\vec{c}, \vec{d}} P' \tto{\vec{e}, \vec{f}} P_2$ and $\vec{d}; \vec{f}$ is equivalent to $\vec{b}$.\end{lemma}
\begin{proof}By induction on the derivation of $P \tto{\vec{a}, \vec{b}} P_1$. We use Lemma~\ref{noninterference} to justify moving each step by $t$ before all steps by threads other than $t$. This gives us an execution of the form $P \tto{\vec{c}, \vec{d}}_t P' \tto{\vec{e}, \vec{f}} P_2$ in which the steps from $P$ to $P'$ execute a complete instrumentation section. This allows us to conclude that $P \xRightarrow{\vec{c}, \vec{d}}_t P' \tto{\vec{e}, \vec{f}} P_2$.\end{proof}

This lemma allows us to reorder the first completed instrumentation section to the front of an execution. The next lemma allows us to identify the first completed instrumentation section if one exists, and forms the basis for all our reordering reasoning henceforth.
\begin{lemma}\label{next-iexec}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. If $P \tto{\vec{a}, \vec{b}} P'$, then either $P'$ is an instrumented suffix of $P_0$, or there are some $P''$ and $t$ such that $P \xRightarrow{\vec{c}, \vec{d}}_t P'' \tto{\vec{e}, \vec{f}} P'$ and $\vec{b}$ is equivalent to $\vec{d}; \vec{f}$. (What about consistency?)\end{lemma}
\begin{proof}By induction on the derivation of $P \tto{\vec{a}, \vec{b}} P'$. In particular, we must consider the case in which $P'$ is an instrumented suffix of $P_0$, but steps to some $P_2'$ that is not an instrumented suffix of $P_0$. In this case, the thread $t$ that advanced in this step must have completed the instrumentation section for some instruction. By Lemma~\ref{first-finished}, we can reorder the steps by $t$ from $P$ to $P_2$ to the front, and obtain a $P''$ such that $P \xRightarrow{\vec{c}, \vec{d}}_t P'' \tto{\vec{e}, \vec{f}} P_2'$ as desired.\end{proof}

We can then inductively apply this reordering to all the completed instrumentation sections in an execution.
\begin{lemma}\label{exec-iexec1}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. If $P \tto{\vec{a}, \vec{b}} P'$, then there is some uninstrumented program $P_1$ such that $P'$ is an instrumented suffix of $P_1$, $P \tTo{\vec{c}, \vec{d}} P_1' \tto{\vec{e}, \vec{f}} P'$, and $\vec{b}$ is equivalent to $\vec{d}; \vec{f}$.\end{lemma}
\begin{proof}By induction on the size of $P$. From Lemma~\ref{next-iexec}, we know that either $P'$ is an instrumented suffix of $P_0$ or we can reorder some instrumentation to the front of the execution. In the former case, we can choose $P_1 = P_0$ and $\vec{c}, \vec{d} = \cdot$ and the rest follows immediately. In the latter case, $P \xRightarrow{\vec{c}, \vec{d}}_t P'' \tto{\vec{e}, \vec{f}} P'$ and $P''$ is smaller than $P$, so by the inductive hypothesis $P'' \tTo{\vec{g}, \vec{h}} P_1' \tto{\vec{i}, \vec{j}} P'$. By combining the big steps, we get the desired execution.\end{proof}

In an execution in which every thread terminates, we can reorder the entire execution into successful handlers:
\begin{lemma}\label{exec-iexec}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. If $P \tto{\vec{a}, \vec{b}} P'$ where $P'$ is some final state, then $P \tTo{\vec{c}, \vec{d}} P'$ and $\vec{b}$ is equivalent to $\vec{d}$.\end{lemma}
\begin{proof}By Lemma~\ref{exec-iexec1}, there is some $P_1$ such that $P \tTo{\vec{c}, \vec{d}} P_1' \tto{\vec{e}, \vec{f}} P'$ and $P'$ is an instrumented suffix of $P_1$. But a final state is only the instrumented suffix of a final state, and so $P_1$ and hence $P_1'$ must also be final. Since $P_1'$ is final, $P_1' = P'$ and the tail end of small steps must be empty, completing the proof.\end{proof}

We must also consider the case in which the instrumented program fails an assertion before reaching a final state. In this case, other threads may be in the middle of executing instrumentation sections when the execution terminates. We begin by defining a relation in which an instrumentation section fails in one step, as shown in Figure~?. Then we can prove the following lemma:
\begin{lemma}\label{exec-fail-iexec}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. If $P \tto{\vec{a}, \vec{b}} \mathit{err}$, then there exists some $P'$ such that $P \tTo{\vec{c}, \vec{d}} P' \xRightarrow{\vec{e}, \vec{f}} \mathit{err}$.\end{lemma}
\begin{proof}There must be some last good state $P''$ such that $P \tto{\vec{a}, \vec{b}} P'' \rightarrow \mathit{err}$. Then by Lemma~\ref{exec-iexec1}, there is some $P_1$ such that $P''$ is an instrumented suffix of $P_1$ and $P \tTo{\vec{c}, \vec{d}} P_1 \tto{\vec{e}, \vec{f}} P''$. Adding the failing step to the end of this execution yields the desired result.\end{proof}

Using Lemmas~\ref{iexec-exec}, \ref{exec-iexec}, and \ref{exec-fail-iexec}, we can reason entirely in terms of the bigger-step relation in which instrumentation executes atomically. At this point, the correctness of the instrumentation becomes a matter of simulation: we need only prove that there is a simulation relation relating states of the uninstrumented and the instrumented program such that they mirror each other's behavior. \TODO{Describe simulation relations here.}

Let $\lfloor\vec{a}\rfloor$ be the list obtained by removing all operations to metadata location from $\vec{a}$. Then the correctness of the instrumentation is expressed by the following theorems:
\begin{theorem}\label{safe}For all well-formed programs $P$, $\instr{t_0}{P} \xrightarrow{\vec{a}, \vec{b}} P_f$ for some final state $P_f$ iff $P \xrightarrow{\vec{a}', \lfloor\vec{b}\rfloor} P_f'$ and $\vec{a}'$ is race-free.\end{theorem}

\begin{theorem}\label{race}For all well-formed programs $P$, $\instr{t_0}{P} \xrightarrow{\vec{a}, \vec{b}} \mathit{err}$ iff $P \xrightarrow{\vec{a}', \vec{b}'} P_f'$ and $\vec{a}'$ contains a race.\end{theorem}

\section{Related Work}

\TODO{discuss verified instrumentation like SoftBound}

There is a rich history of systems that provide dynamic data race detection, dating back to the original proposals of the vector clock algorithm \cite{friedmann,mattern,lamportvc}. Subsequent systems have implemented vector clocks, with various optimizations, using dynamic analysis to provide race detection for C/C++ \cite{pozniansky_efficient_2003,serebryany_threadsanitizer:_2009} and Java \cite{christiaens_trade:_2001,elmas_goldilocks:_2007,flanagan_fasttrack:_2009,flanagan_fasttrack:_2010,slimstate} programs. These systems have relied on paper proofs to demonstrate correctness, but these proofs have not been mechanically verified until our work. Furthermore, these paper proofs operate at a high level of abstraction, using an operational semantics that elides important details such as the synchronization used within the race detector itself. Thus, the implementations of these algorithms are far removed from the algorithms themselves, leaving the door open for bugs.

Lockset-based race detection \cite{dinning_detecting_1991,savage_eraser:_1997}, an alternative to the traditional vector clock data race detection algorithm, reports false races on some common programming idioms like privatization but can also detect with a single execution some races that would require multiple executions to detect with vector clocks. Other work has generalized vector clock race detection to detect more races from a single execution \cite{smaragdakis_sound_2012,sen_detecting_2005,chen_parametric_2007} at the cost of decreased performance.

Several forms of sampling-based dynamic race detection have been proposed. Such schemes trade soundness \cite{greathouse_demand-driven_2011,bond_pacer:_2010,marino_literace:_2009,erickson_effective_2010,effinger-dean_ifrit:_2012} for reduced performance overheads.

There have been several proposals for static race detection \cite{engler_racerx:_2003,naik_effective_2006} or static analysis \cite{flanagan_redcard:_2013,joserenau} to prune race detection instrumentation and metadata at compile time, which can serve as a complement to our dynamic approach. Others have proposed type systems \cite{abadi_types_2006,bocchino_type_2009,rust} and implicitly-parallel languages \cite{rinard_design_1998,guy_blelloch_nesl:_1992} that eliminate data races by construction, though these systems sacrifice expressiveness to obtain race-freedom guarantees.

Several systems exist for detecting data races in structured parallel programs such as fork-join programs \cite{john_mellor-crummey_--fly_1991,feng_efficient_1997,mai_zheng_grace:_2011,michael_boyer_automated_2008}, async-finish programs \cite{raman_scalable_2012} or programs with asynchronous callbacks \cite{petrov_race_2012,raychev_effective_2013,hsiao_race_2014,vechev_oopsla_2015}. Structured parallelism admits more time- and space-efficient data race detection than the general multithreaded programs we support. None of these prior algorithms or implementations have been formally verified, however, so they represent a potentially fruitful area for future work.


\section{Conclusions and Future Work}
conclusions/summary

We intend to expand our approach to verified race detection along three main lines. First, our approach should generalize easily to verifying more sophisticated algorithms and instrumentation passes, such as an implementation of FastTrack~\cite{fasttrack} or the ThreadSanitizer algorithm used in LLVM~\cite{tsan}. The second and greater challenge will be to apply the same approach on more realistic languages, and ultimately to integrate it into high-assurance compilation frameworks like Vellvm~\cite{vellvm} or CompCert~\cite{compcert}. This would involve taking into account a wider range of program instructions and potential complications, including variable-size accesses and low-level atomics. Thirdly, our proofs thus far assume sequential consistency as the concurrent memory model; an interesting challenge would be to prove that the same instrumentation suffices under the relaxed memory models used in languages like C and Java \TODO{cite memory models?}. By extending the reach of verified race detection, we aim to make it easier to design and justify increasingly sophisticated race detection algorithms and implementations.

other kinds of instrumentation, e.g. atomicity violation monitoring

\bibliographystyle{abbrvnat}
\bibliography{sources}

% !! The bibliography should be embedded for final submission.

\end{document}
