\documentclass[preprint, 10pt]{sigplanconf}

\usepackage{uri}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{semantic}
\usepackage{graphicx}
\usepackage{cases}
\usepackage{xcolor}
\usepackage{xspace}

\newcommand{\TODO}[1]{\textbf{\textcolor{red}{[ TODO: #1]}}}
\newcommand{\ignore}[1]{}
\newcommand{\con}[1]{\ensuremath{\mathsf{consistent}(#1)}}
\newcommand{\seqcon}[1]{\ensuremath{\mathsf{seq\_con}(#1)}}
\newcommand{\mread}[2]{\ensuremath{\mathsf{read}(#1, #2)}}
\newcommand{\mwrite}[2]{\ensuremath{\mathsf{write}(#1, #2)}}
\newcommand{\malloc}[2]{\ensuremath{\mathsf{alloc}(#1, #2)}}
\newcommand{\mfree}[1]{\ensuremath{\mathsf{free}(#1)}}
%\newcommand{\cccando}[2]{\ensuremath{\mathsf{can\_do_{CC}}(#1, #2)}}
\newcommand{\hb}[0]{<_{\mathrm{hb}}}
\newcommand{\po}[0]{<_{\mathrm{po}}}
\newcommand{\sw}[0]{<_{\mathrm{sw}}}
\newcommand{\word}[0]{<_{\mathrm{w}}}

\newcommand{\assign}[2]{#1\ \texttt{:=}\ #2}
\newcommand{\load}[2]{#1\ \texttt{:= load}\ #2}
\newcommand{\store}[2]{\texttt{store}\ #2\ #1}
\newcommand{\lock}[1]{\texttt{lock}\ #1}
\newcommand{\unlock}[1]{\texttt{unlock}\ #1}
\newcommand{\spawn}[2]{\texttt{spawn}\ #1\ #2}
\newcommand{\wait}[1]{\texttt{wait}\ #1}
\newcommand{\assert}[2]{\texttt{assert(}#1\ \texttt{<=}\ #2\texttt{)}}
\newcommand{\move}[2]{\ensuremath{\mathsf{move}(#1, #2)}}
\newcommand{\setvc}[2]{\ensuremath{\mathsf{set}(#1, #2)}}
\newcommand{\incvc}[2]{\ensuremath{\mathsf{inc}(#1, #2)}}
\newcommand{\maxa}[2]{\ensuremath{\mathsf{max}(#1, #2)}}
\newcommand{\maxvc}[2]{\ensuremath{\mathsf{merge}(#1, #2)}}
\newcommand{\lea}[2]{\ensuremath{\mathsf{lea}(#1, #2)}}
\newcommand{\vcle}[2]{\ensuremath{\mathsf{hb\_check}(#1, #2)}}
\newcommand{\instr}[2]{\ensuremath{\mathsf{instrument}(#1, #2)}}
\newcommand{\instrp}[1]{\ensuremath{\mathsf{instrument}(#1)}}

%from StackExchange, a better xrightarrow^*
\newcommand{\tto}[1]{\mathrel{
  \vphantom{\xrightarrow{#1}}
  \smash{\xrightarrow{#1}}
  \vphantom{\to}^{*}}
}
\newcommand{\tTo}[1]{\mathrel{
  \vphantom{\xrightarrow{#1}}
  \smash{\xRightarrow{#1}}
  \vphantom{\to}^{*}}
}

\hyphenation{Comp-Cert}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{principle}{Principle}


\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

%\conferenceinfo{PLDI '16}{June 13--17, 2016, Santa Barbara, California, United States} 
%\copyrightyear{2016}
%\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
%\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{}        % These are ignored unless
\preprintfooter{}   % 'preprint' option specified.

\title{Verifying Dynamic Race Detection}
\ignore{\authorinfo{William Mansky \and Yuanfeng Peng \and Steve Zdancewic \and Joseph Devietti}
           {University of Pennsylvania}
           {wmansky@seas.upenn.edu, yuanfeng@cis.upenn.edu, stevez@cis.upenn.edu, devietti@cis.upenn.edu}}
\authorinfo{}{}{}
\maketitle

\begin{abstract}
Writing race-free concurrent code is notoriously difficult, and races can result in bugs that are difficult to isolate and reproduce. Dynamic race detection is often used to catch races that cannot (easily) be detected statically. One approach to dynamic race detection is to instrument the potentially racy code with operations that store and compare metadata, where the metadata implements some known race detection algorithm (e.g. vector clock race detection). In this paper, we lay out an instrumentation pass for race detection in a simple language, and present a mechanized formal proof of its correctness: all races in a program will be caught by the instrumentation, and all races detected by the instrumentation are possible in the original program.
\end{abstract}

%\category{CR-number}{subcategory}{third-level}

%\keywords
%dynamic race detection, interactive theorem proving

\section{Introduction}

Multicore processors have steadily invaded a broad swathe of the computing ecosystem in everything from datacenters to smart watches. Writing multithreaded code for such platforms can bring good performance but also a host of programmability challenges. One of the key challenges is dealing with data races, as the presence of a data race introduces non-sequentially-consistent \cite{manson_java_2005} and in some cases undefined \cite{boehm_foundations_2008} behavior into programs, making program semantics very difficult to understand. Races are not detected by default in current language runtimes, though there are many systems that provide sound and complete data race detection via dynamic analysis \cite{djit+,fasttrack,slimstate,slimfast} to help programmers detect and remove data races from their programs. 

While these analyses have been proven correct, such proofs have two main shortcomings: they are proofs of the algorithms, instead of the implementations, and they are paper proofs instead of machine-checked proofs. Because of these shortcomings, it is possible that a race detector does not faithfully implement its algorithm, or that the algorithm itself is not fully correct. 

In this paper, we seek to rectify these concerns and place dynamic race detection on a provably-correct foundation for the first time. We have begun by formalizing the proof of the classic vector clock race detection algorithm \cite{fidge,friedmann_mattern} using the Coq interactive theorem prover \cite{coq}. Having established the correctness of this base algorithm, we extend our work along two dimensions. 

\TODO{figure showing these two dimensions?}

We first explore the \textbf{algorithmic dimension}, by formally establishing the correctness of the FastTrack algorithm \cite{fasttrack}. FastTrack includes several significant optimizations over the base vector clock algorithm. We find that the correctness of FastTrack can be demonstrated by proving its equivalence to the vector clock algorithm, which is a more straightforward process than demonstrating correctness in isolation, and likely lends itself to formalizing additional algorithms with reduced effort. Our verification efforts have revealed a small issue in the paper proof from \cite{fasttrack} that, while fixable, illustrates the potential dangers that stem from best-effort proofs. We have repaired this issue in our proof of FastTrack, establishing that the algorithm is correct.

We next explore the \textbf{implementation dimension}, by formally establishing in Coq the correctness of an implementation of vector clock race detection on a simple imperative language with threads. Given a program written in our language, our race detector adds instrumentation, written in the same language, to the program to perform vector clock race detection. We demonstrate that this instrumentation correctly implements the vector clock algorithm, again leveraging our previous verification effort. We also prove that our implementation preserves the program's semantics in the absence of races. While constructing our implementation, we consulted the existing implementation of FastTrack for guidance and our verification process brought to light an example of extraneous work performed by the current FastTrack implementation. This issue is unlikely to have come to light otherwise, but when proving our implementation equivalent to the abstract algorithm such discrepancies were quickly revealed. This again demonstrates the value of formal verification over best-effort implementation, ensuring that an implementation does no less, and no more, than is necessary for correctness.

To the best of our knowledge, ours is the first work to adopt formal verification for either race detection algorithms \emph{or} their implementations. Moreover, we believe our general approach may be useful as a template for verifying a broad range of dynamic analyses, especially in the challenging domain of analyses for parallel programs. Verification is critical to help ensure that debugging tools are themselves free from bugs.

This paper makes the following contributions:

\begin{itemize}
\item We present a method for verifying a dynamic data race detector from the algorithmic level through to its implementation
\item We give the first verified proofs of correctness of the vector clock and FastTrack data race detection algorithms
\item We give the first verified implementation of vector clock race detection on a simple, imperative multithreaded language
\item We uncover issues in the paper proof of correctness for FastTrack and in its current implementation, that are unlikely to have been revealed without our verification efforts. We repair these issues in our own proofs and implementation.
\end{itemize}

The remainder of this paper is organized as follows. In Section~\ref{strategy}, we lay out our approach to verifying dynamic race detection algorithms and their implementations. In Section~\ref{algorithms}, we state and verify two algorithms for race detection. In Sections~\ref{language} and \ref{verification}, we describe an instrumentation pass that implements dynamic race detection, and explain the verification process in detail. We compare our approach to related work in Section~\ref{related}, and evaluate our results and describe future work in Section~\ref{conclusion}.

\section{Proof Strategy}
\label{strategy}
Our goal for each algorithm and program transformation presented in this paper is to prove that it implements sound and complete race detection, i.e., that it raises an alarm in all racy executions and only racy executions. However, as much as possible, we prefer not to do this by referring back to the base definition of racy executions. We begin by stating and proving correctness of a simple vector clock race detection algorithm, by direct relation to the definition of a race. We then prove further results---the correctness of a more sophisticated algorithm, and of an instrumentation pass meant to implement the simple algorithm---by relating them to the verified base algorithm. We may think of this hierarchy in terms of specifications and refinement: we begin by proving that the base algorithm refines the abstract specification of race detection, and then use it in turn as a specification refined by more complex or detailed mechanisms. (diagram?) This allows us to separate concerns and avoid duplicating proof effort, but it also serves as further validation of the base vector clock algorithm: by showing that it is \emph{two-sided}, that is, that it both implements a higher-level specification of its desired behavior and is implemented by more concrete systems, we gain confidence that it is correctly stated (and not, e.g., vacuously correct).

Our approach to verifying instrumentation has three major steps. First, we describe our race detection algorithm abstractly, separately from the details of any programming language. We verify this algorithm against a high-level specification of the property we want to guarantee (in this case, soundness and completeness). Secondly, we define the semantics of a target language, labeled with the abstract operations produced by each step. This means that from each execution of an uninstrumented program, we can use the algorithm to determine the behavior we \emph{would have observed} if the program was correctly instrumented. Thirdly, we define our instrumentation pass, and also write a bigger-step semantics for instrumented programs in which an instruction executes together with its instrumentation in a single step. This bigger-step semantics is analogous to that used in SoftBound~\cite{softbound}, and makes it easier to directly relate executions of an instrumented program to executions of the original program. Given the bigger-step semantics, the proof of correctness of the instrumentation then breaks down into two parts: a proof of simulation between the bigger-step semantics and ``would have observed'' semantics of the uninstrumented program, and a proof that the bigger-step semantics completely captures the possible behaviors of any instrumented program. This three-step approach has applications beyond race detection: we believe that it could be applied to simplify the verification of any kind of instrumentation for dynamic checks, including memory safety (as in SoftBound) and atomicity violation checking. The approach is particularly useful when verifying instrumentation of concurrent programs, since we are able to isolate all reasoning about interference between threads in the latter half of the third step---characterizing the possible behaviors of the instrumented program---and otherwise reason more or less sequentially.

\section{Race Detection Algorithms}
\label{algorithms}

\subsection{Defining Races}
\TODO{We need to start by defining races.}

\subsection{Vector Clock Race Detection}
definition

\subsubsection{Correctness}
The correctness of vector clock race detection follows from the fact that the $\sqsubseteq$ relation between vector clocks precisely models the happens-before relation. Our formalization follows the proof outline given in the presentation of FastTrack~\cite{fasttrack} (with the change described in Section~\ref{bug}), which can be straightforwardly translated into Coq. The main invariant of the algorithm is that $C_t(t) > C_u(t)$, that is, each thread always has a higher timestamp for itself than any other thread has for it.

\subsection{FastTrack}
definition

\subsubsection{Proof by Simulation}
Beyond guaranteeing the correctness of FastTrack, we want to test the simulation approach described in Section~\ref{strategy} as a tool for verifying refinements of the vector clock algorithm. In this section, we describe our novel simulation proof of FastTrack's correctness; in the following section, we describe our mechanization of the paper proof (which directly relates the algorithm to the definition of race).

Intuitively, the metadata optimizations of FastTrack are safe to perform because the reduced metadata still captures the same happens-before relationships, which means that the same $\sqsubseteq$ relationships hold between corresponding state components. Thus, we need only present a relation between FastTrack states and full vector clock states that captures this correspondence in order to prove a bisimulation between the two systems. The $C$ and $L$ components should remain unchanged. To characterize the expected semantics of epochs, we define the following \emph{encapsulation} relation:
\begin{definition}An epoch $e$ \emph{encapsulates} a vector clock $V$ for another vector clock $V'$ if $e \preceq V'$ implies that $V \sqsubseteq V'$. An epoch $e$ encapsulates $V$ in a state $(C, L, R, W)$ if it summarizes $V$ for $C_u$ for all $u$ and $L_m$ for all $m$.\end{definition}

\begin{figure}[htb]
\TODO{Maybe show a program, and the corresponding vector clocks going through read VC, empty epoch, read epoch}
\caption{Relating FastTrack and vector clock states}
\label{encapsulation}
\end{figure}

In FastTrack, when write metadata $W_x$ is collapsed to an epoch $c@t$, it is precisely because $W_x(t) = c$ and $c@t$ encapsulates $W_x$. The relation for read metadata is more complicated, for one particular reason: the Write Shared rule resets read metadata to an empty epoch. This has two effects on the simulation relation. First, we must make a special allowance for the case in which the read metadata is empty; in this case, it is the write epoch that encapsulates the original read vector clock. Second, even when the FastTrack state holds a vector clock in $R_x$, that vector clock may contain some 0 values in places where reads have in fact been performed. In effect, because FastTrack read vector clocks are always derived from epochs, they carry forward a partial encapsulation relation, in which one component encapsulates all the relationships on zeroed values.
\begin{definition}A vector clock $V_0$ \emph{partially encapsulates} a vector clock $V$ for another vector clock $V'$ if there is some thread $t$ such that:
\begin{itemize}
\item for all $u$ such that $V_0(u) = 0$, $V_0(t) \le V'(t)$ implies that $V(u) \le V'(u)$
\item for all $u$ such that $V_0(u) \neq 0$, $V_0(u) \le V'(u)$ implies that $V(u) \le V'(u)$
\end{itemize}
A vector clock $V_0$ partially encapsulates a vector clock $V$ in a state $(C, L, R, W)$ if it partially encapsulates $V$ for $C_u$ for all $u$ and $L_m$ for all $m$.\end{definition}

We can now state the full simulation relation between vector clock and FastTrack states.
\begin{definition}A vector clock state $(C, L, R, W)$ and a FastTrack state $(C', L', R', W')$ are in the relation $\sim$ when $C' = C$, $L' = L$, and for every location $x$:
\begin{itemize}
\item if $W'_x = c@t$, then $W_x(t) = c$ and $c@t$ encapsulates $W_x$
\item $R'_x(t) \le R_x(t)$ for all $t$
\item if $R'_x = \bot_e$, then $W'_x$ encapsulates $R_x$
\item if $R'_x = c@t$, then $R_x(t) = c$ and $c@t$ encapsulates $R_x$
\item if $R'_x = V$, then $V$ partially encapsulates $R_x$
\end{itemize}
\end{definition}
\begin{theorem}The relation $\sim$ is a bisimulation.\end{theorem}
\begin{proof}In each direction, the relation $\sim$ is preserved by corresponding steps in the two systems, which we show by case analysis on the rule applied.\end{proof}

While the simulation relation is complicated by the encapsulation relations, once it is correctly stated, the proof itself is reduced to proving that various $\le$ relationships are preserved by mathematical operations on vector clocks. We expect that the arithmetic involved could be further automated, decreasing the burden of verifying related algorithms.

\subsubsection{Direct Proof}
\label{bug}
Flanagan and Freund justify the optimizations of FastTrack by proving that the $\sqsubseteq$ relation on vector clocks still precisely captures the happens-before relation. As a baseline for comparison, we formalized this proof in Coq as well, and discovered an error in the paper proof in the process. FastTrack's Lemma 4 states the following: {\it Suppose $\sigma$ is well-formed and $\sigma \Rightarrow^\alpha \sigma'$ and $a, b \in \alpha$. Let $t = \mathit{tid}(a)$ and $u = \mathit{tid}(b)$. If $a <_{\alpha} b$ then $K^a(t) \sqsubseteq K^b(t)$.} This lemma is then used to prove that if some operation $b$ is \emph{stuck}, then a previous operation $a$ must have raced with it, since $K^a(t) \not\sqsubseteq K^b(t)$. However, the premise of Lemma 4 requires that $a$ and $b$ not be stuck, since they are in a trace $\alpha$ that successfully executes to a state $\sigma'$. Because of this constraint, Lemma 4 in fact does not apply. Fortunately, this premise is stronger than necessary to prove Lemma 4, and in Coq we are able to prove a more general version:
\begin{lemma}Suppose $\sigma$ is well-formed and $\sigma \Rightarrow^\alpha \sigma'$ and $a \in \alpha$. Let $t = \mathit{tid}(a)$ and $u = \mathit{tid}(b)$. If $a <_{\alpha; b} b$ then $K^a(t) \sqsubseteq K^b(u)$.\end{lemma}
where by abuse of notation we use $K^b(t)$ to refer to the $C_u$ component of $\sigma'$ updated as appropriate for a blocking operation. With this statement of the lemma, the completeness proof follows as outlined in the paper.

Compared to the proof by simulation, this proof is about 130 lines longer, reflecting duplication of effort from proving similar theorems about the two systems. It also involves a significant amount of inductive reasoning, which might have been difficult to synthesize without the guide of the paper proof. In this case, since FastTrack preserves the same invariants as the base algorithm, the proofs have largely the same structure; however, if a different algorithm modified some of these invariants, we believe that the proof by simulation would be significantly easier to construct than the proof that recapitulates the relationship between $\sqsubseteq$ and happens-before.

\section{Instrumenting a Simple Language}
\label{language}

\subsection{The Language}
We define a simple multithreaded language that is just complicated enough to have races and implement race detection instrumentation. The instructions of the language are defined as follows, where $n$ is a natural number, $a$ is a local variable, $e, e1, e2$ are expressions, $p$ is a pointer, $l$ is a lock, $t$ is a numeric thread id, and $i_j$ are instructions:
\begin{align*}\mathit{expr} ::= n~|~a~|~e_1 + e_2~|~\mathtt{max}(e_1, e_2)\end{align*}
\begin{align*}\mathit{instr} ::=\ &\assign{a}{e}~|~\load{a}{p}~|~\store{p}{e}~|~\lock{l}~\\|~&\unlock{l}~|~\spawn{t}{i_1, ..., i_n}~|~\wait{t}~\\|~&\assert{e_1}{e_2}\end{align*}
A \emph{program} is simply a list of instructions, considered to be the body of the main thread with id $0$. The dynamic state of a program contains a collection of threads, where a thread is a pair $(t, \mathit{li})$ of an id and a list of instructions, and a family of local environments $G_t$, one for each thread, assigning values to local variables. We give semantics to the language via a transition system of the form $(P, G) \xrightarrow{o, c} (P', G')$, where $P, P'$ are collections of threads, $G, G'$ are environments, $o$ is a race detection operation (if one is produced by the step), and $c$ is a concurrent memory operation (likewise). We define a function $\mathsf{eval}$ that takes a local environment and an expression and yields the value of the expression. The semantics of the language are shown in Figure~\ref{semantics}. \TODO{I'm not satisfied with this presentation; can we distinguish memory and RD ops better?}

\begin{figure}[htb]
$$\inference[assign]{P(t) = \assign{a}{e}\texttt{;} \mathit{li}}{(P, G) \rightarrow_t \\(P[t \mapsto \mathit{li}], G[G_t[a \mapsto \mathsf{eval}(G_t, e)]])}$$

\caption{Semantics of the simple language}
\label{semantics}
\end{figure}

A state $P$ is \emph{final} if all of its threads have executed to completion.

\subsection{Instrumentation}
For each rule of the vector clock algorithm, we provide a code snippet that can be added to the associated instruction to implement the rule. We begin by defining macros for the basic mathematical operations of the algorithm, as shown in Figure~\ref{helper}. We designate two local variables unused in the base program (\texttt{tmp1} and \texttt{tmp2}) as temporaries for use by the instrumentation. Let $z$ be the largest thread id generated in the lifetime of the program. (Note that in our simple language, $z$ can be determined statically; in real-world implementations, the vector clock data would need to be dynamically resized in memory when this bound is exceeded.)

\begin{figure}[htb]
\move{p}{q} $\triangleq$ \load{\texttt{tmp1}}{$p$}\texttt{;} \store{$q$}{\texttt{tmp1}}

\setvc{a}{b} $\triangleq$ \move{(a, 0)}{(b, 0)}\texttt{;} ...\texttt{;} \move{(a, z - 1)}{(b, z - 1)}
$(V = V')$

\incvc{a}{b} $\triangleq$ \load{\texttt{tmp1}}{$(a, t)$}\texttt{;} \assign{\texttt{tmp1}}{\texttt{tmp1 + 1}}\texttt{;} \store{$(a, t)$}{\texttt{tmp1}}
$(C = C[t := \mathit{inc}_t(C_t)])$

\maxa{p}{q} $\triangleq$ \load{\texttt{tmp1}}{$p$}\texttt{;} \load{\texttt{tmp2}}{$q$}\texttt{;} \assign{\texttt{tmp2}}{\texttt{max\ tmp1\ tmp2}}\texttt{;} \store{$p$}{\texttt{tmp2}}

\maxvc{a}{b} $\triangleq$ \maxa{(a, 0)}{(b, 0)}\texttt{;} ...\texttt{;} \maxa{(a, z - 1)}{(b, z - 1)}
$(C' = C \sqcup C')$

\lea{p}{q} $\triangleq$ \load{\texttt{tmp1}}{$p$}\texttt{;} \load{\texttt{tmp2}}{$q$}\texttt{;} \assert{\texttt{tmp1}}{\texttt{tmp2}}

\vcle{a}{b} $\triangleq$ \lea{(a, 0)}{(b, 0)}\texttt{;} ...\texttt{;} \lea{(a, z - 1)}{(b, z - 1)}
$(C \sqsubseteq C')$
\caption{Helper macros}
\label{helper}
\end{figure}

With these macros as building blocks, we can straightforwardly translate the rules of vector clock race detection \TODO{cite figure} into code. We set aside dedicated areas of memory for each of the components of the vector clock state, labeled $C$, $L$, $R$, and $W$ accordingly, and index into them by assigning a unique numeric identifier to each thread, local variable, and lock, so that the block at $L + m$ in memory contains the vector clock $L_m$. Note that the timing of the associated vector clock operations depends on the instruction being instrumented; in particular, the blocking operations \texttt{lock} and \texttt{wait} must not change the vector clock state until after they successfully complete.

\begin{figure}[htb]
\instr{t}{\load{a}{(b, o)}} $\triangleq$ \vcle{W + b}{C + t}\texttt{;} \move{(C + t, t)}{(R + b, t)}\texttt{;} \load{$a$}{$(b, o)$}

\instr{t}{\store{(b, o)}{e}} $\triangleq$ \vcle{W + b}{C + t}\texttt{;} \vcle{R + b}{C + t}\texttt{;} \move{(C + t, t)}{(W + b, t)}\texttt{;} \store{$(b, o)$}{$e$}

\instr{t}{\lock{m}} $\triangleq$ \lock{$m$}\texttt{;} \maxvc{L + m}{C + t}

\instr{t}{\unlock{m}} $\triangleq$ \setvc{C + t}{L + m}\texttt{;} \incvc{t}{C + t}\texttt{;} \unlock{$m$}

\instr{t}{\spawn{u}{\mathit{li}}} $\triangleq$ \maxvc{C + t}{C + u}\texttt{;} \incvc{t}{C + t}\texttt{;} \spawn{$u$}{(\instr{u}{\mathit{li}})}

\instr{t}{\wait{u}} $\triangleq$ \wait{$u$}\texttt{;} \maxvc{C + u}{C + t}\texttt{;} \incvc{u}{C + u}
\caption{Instrumentation}
\label{instrumentation}
\end{figure}

The instrumented version of a program $P$ is then simply $\instrp{P} \triangleq \instr{0}{P}$. Because in our language the bodies of future threads are embedded in the \texttt{spawn} instructions, we instrument these threads by recursively calling $\mathsf{instrument}$ on their bodies.

\subsection{Necessary Synchronization}
\begin{figure}[htb]
\centering
\begin{tabular}{c || c}
\load{\texttt{tmp1}}{$(W + b, 0)$} & \load{\texttt{tmp1}}{$(W + b, 0)$}\\
... & ...\\
& \load{\texttt{tmp1}}{$(R + b, 0)$}\\
... & ...\\
\store{$(R + b, 0)$}{\texttt{tmp1}} & \store{$(W + b, 0)$}{\texttt{tmp1}}\\
... & ...\\
\load{\texttt{a}}{$(b, o)$} & \store{$(b, o)$}{\texttt{2}}
\end{tabular}

\caption{Races in instrumentation}
\label{instr-race}
\end{figure}

The instrumentation of the previous section cannot implement sound and complete race detection for one important reason: when a race does occur, the corresponding instrumentation also races. In instrumented programs such as that shown in Figure~\ref{instr-race}, depending on the order in which the updates to metadata locations occur, the instrumentation may fail to detect the race between the two threads. In general, poorly synchronized race detection instrumentation cannot hope to successfully detect all races. At the same time, adding too much synchronization could significantly hurt performance. Given the set of operations available in our language, we can show that it suffices to add a lock for each memory location, which is used to protect the instrumentation on that memory location. (Intuitively, locks prevent races on their associated metadata, and a thread cannot race with the thread that spawns it or waits for it to terminate.) To implement the necessary synchronization in our instrumentation, we add another designated area of memory, $X$, such that $X + b$ holds the lock protecting the metadata for block $b$. We then add locking to the \texttt{load} and \texttt{store} instrumentation:
\begin{align*}&\instr{t}{\load{a}{(b, o)}} \triangleq \lock{X + x}\texttt{;} \\&\quad\vcle{W + b}{C + t}\texttt{;} \move{(C + t, t)}{(R + b, t)}\texttt{;} \\&\quad\load{a}{(b, o)}\texttt{;} \unlock{X + x}\end{align*}
\begin{align*}&\instr{t}{\store{(b, o)}{e}} \triangleq \lock{X + x}\texttt{;} \\&\quad\vcle{W + b}{C + t}\texttt{;} \vcle{R + b}{C + t}\texttt{;} \\&\quad\move{(C + t, t)}{(W + b, t)}\texttt{;} \store{(b, o)}{e}\texttt{;} \unlock{X + x}\end{align*}
This guarantees that the instrumentation will never race with itself, which is sufficient to allow us to prove correctness of the instrumentation in the next section.

\label{waits}
The semantics of our language allow one more case in which instrumentation may race. Since a terminated thread is not removed from the state when another thread \texttt{wait}s for it, two threads may \texttt{wait} for the same thread, and the subsequent race detection operations may conflict. While this is a fairly obscure case, on closer examination, it is also preventable. The $\mathsf{merge}$ operation only reads the vector clock of the terminated thread, and simultaneous reads to the same location do not constitute races. The only write to $C + u$ is in the $\mathsf{inc}$ macro, which is performed to maintain the invariant that a thread always has a higher timestamp for itself than any other thread has for it. While this simplifies reasoning, the invariant is not actually necessary for a thread that will perform no further operations, which must be the case for the target of a \texttt{wait}. If we remove the $\mathsf{inc}$ from the \texttt{wait} instrumentation, we get an equally correct race detection algorithm that allows for multiple \texttt{waits} on a single thread \TODO{proof?}.

\section{Verifying Instrumentation}
\label{verification}
We verify the correctness of the instrumentation by showing that instrumentation records the same information and performs the same checks as the vector clock algorithm would perform on the input program.

Key to our correctness proof is the idea that the instrumented program can be thought of as executing under a bigger-step semantics in which each instruction and its instrumentation execute in a single step. This semantics is shown in Figure~\TODO{?}
\begin{lemma}\label{iexec-exec}If $(P, G) \xRightarrow{\vec{a}, \vec{b}}_t (P', G')$, then $(P, G) \xrightarrow{\vec{a}, \vec{b}}_t (P', G')$.\end{lemma}
\begin{proof}By case analysis and application of the relevant small-step rules.\end{proof}

We would also like to prove the correspondence in the other direction, but this is much more difficult. Any given execution of an instrumented program may not line up with one in which the instrumentation for each instruction executes in a single step; at a state in the middle of the execution, the program may be in the process of executing as many different instrumentation sections as there are threads. We resolve this problem by showing that we can ``reorder'' the steps of any execution so that the instrumentation for each instruction executes contiguously. More precisely, we show that each execution of an instrumented program is equivalent to one in which the instrumentation executes atomically.

The reordering proceeds inductively: given an execution, we can always find the first complete instrumentation section, move its steps to the front of the execution, and then continue on the remaining steps. We begin by defining the uninstrumented state represented by each intermediate state of the execution.
\begin{definition}An instrumented state $P$ is an \emph{instrumented suffix} of a state $P_0$ if $P_0$ and $P$ contain the same threads and for each thread $t$, $P(t)$ is obtained by dropping some prefix of the instrumentation of the first instruction from $\instr{t}{P_0(t)}$ (or is empty if $P_0(t)$ is empty).\end{definition}

Steps by a thread have no effect on the state or local environment of other threads. The only way in which they communicate is via their effects on the shared memory. As such, the main obligation in proving that we can reorder steps in an execution is to show that reordering the associated memory operations does not change the behavior of the program. For our purposes, it suffices to show the stronger condition that if two instrumentation sections execute simultaneously, then the memory locations that they access do not overlap. We refer to this property as \emph{noninterference}. In the following, we use $P \rightarrow^{*}_t P'$ to mean that there is a (possibly empty) sequence of steps $P \rightarrow_t P_1 \rightarrow_t ... \rightarrow_t P'$, and $P \rightarrow^{*}_{\not t} P'$ to mean that there is a (possibly empty) sequence of steps $P \rightarrow_a P_1 \rightarrow_b ... \rightarrow_z P'$ where $a, b, ..., z \neq t$.

\begin{lemma}\label{indep}Let $P_0$ be a well-formed uninstrumented program and $P_0'$ its instrumented counterpart. Suppose we have some state $P'$ and instruction $i$ such that $P_0' \rightarrow^{*} P'$ and $P'(t) = \instr{t}{i}; \mathit{li}$, and $P' \xrightarrow{o1, c1}_t P_1 \tto{\vec{a}, \vec{b}} P_2 \xrightarrow{o2, c2}_u P_3$ such that $P_2(t) = i'; ...; \mathit{li}$. Then the operations performed by $t$ in $c1; \vec{b}; c2$ do not overlap with the operations performed by threads other than $t$ in $c1; \vec{b}; c2$.\end{lemma}
\begin{proof}By case analysis on the instruction $i$. In the cases of assignment and \texttt{assert} statements, no memory operations are produced. For \texttt{load} and \texttt{store} instructions on a location $(b, o)$, the lock $X + b$ protects all associated memory locations, and so no other thread can access them until the lock is released. For \texttt{lock} and \texttt{unlock} instructions on a lock $m$, $m$ itself protects its associated metadata, and likewise no other thread can access until the instrumentation is finished and the lock released. For \texttt{spawn} instructions, the thread is not spawned until the end of the instrumentation, and so no other thread can interact with it or its metadata. Likewise, for \texttt{wait} instructions, the instrumentation cannot begin to execute until the target of the \texttt{wait} has terminated, and at that point no other thread will access its metadata (we assume that only one thread \texttt{wait}s for each thread; see the note in Section~\ref{waits}).\end{proof}

This lemma lets us gather up the steps by a thread $t$ that make up an instrumentation section, and reorder them into a consecutive sequence.
\begin{lemma}[Noninterference]\label{noninterference}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. Suppose that $P \tto{\vec{a}, \vec{b}}_t P_1 \tto{\vec{c}, \vec{d}}_{\not t} P_2 \xrightarrow{o, c}_t P_3$ such that $P_2$ is an instrumented suffix of $P_0$. Then $c$ does not overlap with the operations in $\vec{d}$.\end{lemma}
\begin{proof}By induction on the derivation of $P_1 \tto{\vec{c}, \vec{d}}_{\not t} P_2$. Since $P_2$ is an instrumented suffix of $P_0$, we know that no instrumentation section is completed in this section of the execution. Thus, each step by a thread $u \neq t$ is either the first step of an instrumentation section, or somewhere in the middle of an instrumentation section. In the former case, we know from Lemma~\ref{indep} that the following operations in $\vec{d}; c$ by threads other than $u$ do not overlap with the operations by $u$. In the latter case, there must have previously existed a first step by $u$ that began executing the instrumentation section, and again we know from Lemma~\ref{indep} that all operations in $\vec{d}; c$ by threads other than $u$ do not overlap with operations by $u$. We can conclude that the operations by each thread in $\vec{d}; c$ are to disjoint locations. Since none of the operations in $\vec{d}$ are by $t$, this means that $c$ does not overlap with any of the operations in $\vec{d}$.\end{proof}
\begin{lemma}\label{first-finished}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. Suppose that $P \tto{\vec{a}, \vec{b}} P_1 \xrightarrow{o, c}_t P_2$, where $P_1$ is an instrumented suffix of $P_0$ and the step from $P_1$ to $P_2$ completes an instrumentation section. Then there exists a state $P'$ such that $P \xRightarrow{\vec{c}, \vec{d}} P' \tto{\vec{e}, \vec{f}} P_2$ and $\vec{d}; \vec{f}$ is equivalent to $\vec{b}$.\end{lemma}
\begin{proof}By induction on the derivation of $P \tto{\vec{a}, \vec{b}} P_1$. We use Lemma~\ref{noninterference} to justify moving each step by $t$ before all steps by threads other than $t$. This gives us an execution of the form $P \tto{\vec{c}, \vec{d}}_t P' \tto{\vec{e}, \vec{f}} P_2$ in which the steps from $P$ to $P'$ execute a complete instrumentation section. This allows us to conclude that $P \xRightarrow{\vec{c}, \vec{d}}_t P' \tto{\vec{e}, \vec{f}} P_2$.\end{proof}

This lemma allows us to reorder the first completed instrumentation section to the front of an execution. The next lemma allows us to identify the first completed instrumentation section if one exists, and forms the basis for all our reordering reasoning henceforth.
\begin{lemma}\label{next-iexec}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. If $P \tto{\vec{a}, \vec{b}} P'$, then either $P'$ is an instrumented suffix of $P_0$, or there are some $P''$ and $t$ such that $P \xRightarrow{\vec{c}, \vec{d}}_t P'' \tto{\vec{e}, \vec{f}} P'$ and $\vec{b}$ is equivalent to $\vec{d}; \vec{f}$. (What about consistency?)\end{lemma}
\begin{proof}By induction on the derivation of $P \tto{\vec{a}, \vec{b}} P'$. In particular, we must consider the case in which $P'$ is an instrumented suffix of $P_0$, but steps to some $P_2'$ that is not an instrumented suffix of $P_0$. In this case, the thread $t$ that advanced in this step must have completed the instrumentation section for some instruction. By Lemma~\ref{first-finished}, we can reorder the steps by $t$ from $P$ to $P_2$ to the front, and obtain a $P''$ such that $P \xRightarrow{\vec{c}, \vec{d}}_t P'' \tto{\vec{e}, \vec{f}} P_2'$ as desired.\end{proof}

We can then inductively apply this reordering to all the completed instrumentation sections in an execution.
\begin{lemma}\label{exec-iexec1}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. If $P \tto{\vec{a}, \vec{b}} P'$, then there is some uninstrumented program $P_1$ such that $P'$ is an instrumented suffix of $P_1$, $P \tTo{\vec{c}, \vec{d}} P_1' \tto{\vec{e}, \vec{f}} P'$, and $\vec{b}$ is equivalent to $\vec{d}; \vec{f}$.\end{lemma}
\begin{proof}By induction on the size of $P$. From Lemma~\ref{next-iexec}, we know that either $P'$ is an instrumented suffix of $P_0$ or we can reorder some instrumentation to the front of the execution. In the former case, we can choose $P_1 = P_0$ and $\vec{c}, \vec{d} = \cdot$ and the rest follows immediately. In the latter case, $P \xRightarrow{\vec{c}, \vec{d}}_t P'' \tto{\vec{e}, \vec{f}} P'$ and $P''$ is smaller than $P$, so by the inductive hypothesis $P'' \tTo{\vec{g}, \vec{h}} P_1' \tto{\vec{i}, \vec{j}} P'$. By combining the big steps, we get the desired execution.\end{proof}

In an execution in which every thread terminates, we can reorder the entire execution into successful handlers:
\begin{lemma}\label{exec-iexec}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. If $P \tto{\vec{a}, \vec{b}} P'$ where $P'$ is some final state, then $P \tTo{\vec{c}, \vec{d}} P'$ and $\vec{b}$ is equivalent to $\vec{d}$.\end{lemma}
\begin{proof}By Lemma~\ref{exec-iexec1}, there is some $P_1$ such that $P \tTo{\vec{c}, \vec{d}} P_1' \tto{\vec{e}, \vec{f}} P'$ and $P'$ is an instrumented suffix of $P_1$. But a final state is only the instrumented suffix of a final state, and so $P_1$ and hence $P_1'$ must also be final. Since $P_1'$ is final, $P_1' = P'$ and the tail end of small steps must be empty, completing the proof.\end{proof}

We must also consider the case in which the instrumented program fails an assertion before reaching a final state. In this case, other threads may be in the middle of executing instrumentation sections when the execution terminates. We begin by defining a relation in which an instrumentation section fails in one step, as shown in Figure~?. Then we can prove the following lemma:
\begin{lemma}\label{exec-fail-iexec}Let $P_0$ be a well-formed uninstrumented program and $P$ its instrumented counterpart. If $P \tto{\vec{a}, \vec{b}} \mathit{err}$, then there exists some $P'$ such that $P \tTo{\vec{c}, \vec{d}} P' \xRightarrow{\vec{e}, \vec{f}} \mathit{err}$.\end{lemma}
\begin{proof}There must be some last good state $P''$ such that $P \tto{\vec{a}, \vec{b}} P'' \rightarrow \mathit{err}$. Then by Lemma~\ref{exec-iexec1}, there is some $P_1$ such that $P''$ is an instrumented suffix of $P_1$ and $P \tTo{\vec{c}, \vec{d}} P_1 \tto{\vec{e}, \vec{f}} P''$. Adding the failing step to the end of this execution yields the desired result.\end{proof}

Using Lemmas~\ref{iexec-exec}, \ref{exec-iexec}, and \ref{exec-fail-iexec}, we can reason entirely in terms of the bigger-step relation in which instrumentation executes atomically. At this point, the correctness of the instrumentation becomes a matter of simulation: we need only prove that there is a bisimulation relation relating states of the uninstrumented and the instrumented program such that they mirror each other's behavior. For each of the two directions of bisimulation, we must consider the case in which the original program does not race (and the instrumented program matches its behavior), and the case in which it does race (and the instrumented program fails an assertion). We begin by defining the relationship between states of the abstract algorithm and memory configurations.
\begin{definition}A block $b$ in a memory $m$ encodes a vector clock $V$ if for all $t \le z$, the value at $(b, t)$ in $m$ is equal to $V_t$. A vector clock state $(C, L, R, W)$ is modeled by a memory $m$, written $m \models (C, L, R, W)$, if $C + t$ encodes $C_t$ for every $t$, $L + m$ encodes $L_m$ for every $m$, and $R + b$ and $W + b$ encode $R_b$ and $W_b$ respectively for every $b$.\end{definition}

\begin{definition}The relation $\sim$ relates an uninstrumented configuration $(P, G, m)$ to an instrumented configuration $(P', G', m')$ if:
\begin{itemize}
\item $P' = \instr{0}{P}$
\item for all $t$ and all $a$ other than \texttt{tmp1} and \texttt{tmp2}, $G'_t(a) = G_t(a)$
\item $m$ and $m'$ hold the same values at all non-metadata locations
\end{itemize}
\end{definition}
\begin{lemma}If $P$ is a well-formed program, $(P, G, m) \sim (P', G', m')$, $m' \models s$, $(P, G, m) \xrightarrow{o, c}_t (P_2, G_2, m_2)$, and $s \xrightarrow{o} s'$, then $(P', G', m') \xRightarrow{\vec{o'}, \vec{c'}}_t (P_2', G_2', m_2')$ such that $(P_2, G_2, m_2) \sim (P_2', G_2', m_2')$ and $m_2' \models s'$.\end{lemma}
\begin{lemma}If $P$ is a well-formed program, $(P, G, m) \sim (P', G', m')$, $m' \models s$, $(P, G, m) \xrightarrow{o, c}_t (P_2, G_2, m_2)$, and $s \not\xrightarrow{o}$, then $(P', G', m') \xRightarrow{\vec{o'}, \vec{c'}}_t \mathit{err}$.\end{lemma}
\begin{lemma}If $P$ is a well-formed program, $(P, G, m) \sim (P', G', m')$, $m' \models s$, and $(P', G', m') \xRightarrow{\vec{o}, \vec{c}}_t (P_2', G_2', m_2')$, then $m_2' \models s'$ and $(P, G, m) \xrightarrow{o', c'}_t (P_2, G_2, m_2)$ such that $(P_2, G_2, m_2) \sim (P_2', G_2', m_2')$ and $s \xrightarrow{o'} s'$.\end{lemma}
\begin{lemma}If $P$ is a well-formed program, $(P, G, m) \sim (P', G', m')$, $m \models s$, and $(P, G, m) \xRightarrow{\vec{o}, \vec{c}}_t \mathit{err}$, then $(P, G, m) \xrightarrow{o', c'}_t (P_2, G_2, m_2)$ and $s \not\xrightarrow{o'}$.\end{lemma}

The correctness of the instrumentation is expressed by the following theorems:
\begin{theorem}\label{safe}For all well-formed programs $P$, $(\instrp{P}, G_0) \xrightarrow{\vec{a}', \vec{b}'} (P_f', G_f')$ for some final state $P_f$ iff $(P, G_0) \xrightarrow{\vec{a}, \vec{b}} (P_f', G_f')$, $\vec{a}'$ is race-free, and $(P_f, G_f, m_0; \vec{b}) \sim (P_f', G_f', m_0; \vec{b}')$.\end{theorem}

\begin{theorem}\label{race}For all well-formed programs $P$, $(\instrp{P}, G_0) \xrightarrow{\vec{a}', \vec{b}'} (P_1', G_1') \rightarrow \mathit{err}$ iff $(P, G_0) \xrightarrow{\vec{a}, \vec{b}} (P_1, G_1) \xrightarrow{o, c}_t (P_2, G_2)$, $(P_1, G_1, m_0; \vec{b}) \sim (P_1', G_1', m_0; \vec{b}')$, $\sigma_0 \xrightarrow{\vec{a}} \sigma$, and $\sigma \not\xrightarrow{o}$.\end{theorem}
Since $(P, G, m) \sim (P', G', m')$ implies that $G$ and $G'$, and likewise $m$ and $m'$, agree on all locations except those involved in the instrumentation, these are strong correctness properties. Theorem~\ref{safe} guarantees that for each race-free execution of the original program, there is a successful instrumented execution that produces the same values in the environment and memory (and vice versa). Theorem~\ref{race} guarantees that for each racy execution of the original program, there is a corresponding instrumented execution that executes successfully up until the first race, then fails (and vice versa). While it is difficult to talk about ``the same'' execution across two different programs, these lemmas guarantee that in terms of observable results, the original and instrumented programs have the same behavior modulo race detection, and that all racy executions are successfully detected.

\section{Related Work}
\label{related}

\TODO{discuss verified instrumentation like SoftBound, RockSalt SFI? I don't think either of these support multithreading.}

There is a rich history of systems that provide dynamic data race detection, dating back to the original proposals of the vector clock algorithm \cite{friedmann,mattern,lamportvc}. Subsequent systems have implemented vector clocks, with various optimizations, using dynamic analysis to provide race detection for C/C++ \cite{pozniansky_efficient_2003,serebryany_threadsanitizer:_2009} and Java \cite{christiaens_trade:_2001,elmas_goldilocks:_2007,flanagan_fasttrack:_2009,flanagan_fasttrack:_2010,slimstate} programs. These systems have relied on paper proofs to demonstrate correctness, but these proofs have not been mechanically verified until our work. Furthermore, these paper proofs operate at a high level of abstraction, using an operational semantics that elides important details such as the synchronization used within the race detector itself. Thus, the implementations of these algorithms are far removed from the algorithms themselves, leaving the door open for bugs.

Lockset-based race detection \cite{dinning_detecting_1991,savage_eraser:_1997}, an alternative to the traditional vector clock data race detection algorithm, reports false races on some common programming idioms like privatization but can also detect with a single execution some races that would require multiple executions to detect with vector clocks. Other work has generalized vector clock race detection to detect more races from a single execution \cite{smaragdakis_sound_2012,sen_detecting_2005,chen_parametric_2007} at the cost of decreased performance.

Several forms of sampling-based dynamic race detection have been proposed. Such schemes trade soundness \cite{greathouse_demand-driven_2011,bond_pacer:_2010,marino_literace:_2009,erickson_effective_2010,effinger-dean_ifrit:_2012} for reduced performance overheads.

There have been several proposals for static race detection \cite{engler_racerx:_2003,naik_effective_2006} or static analysis \cite{flanagan_redcard:_2013,joserenau} to prune race detection instrumentation and metadata at compile time, which can serve as a complement to our dynamic approach. Others have proposed type systems \cite{abadi_types_2006,bocchino_type_2009,rust} and implicitly-parallel languages \cite{rinard_design_1998,guy_blelloch_nesl:_1992} that eliminate data races by construction, though these systems sacrifice expressiveness to obtain race-freedom guarantees.

Several systems exist for detecting data races in structured parallel programs such as fork-join programs \cite{john_mellor-crummey_--fly_1991,feng_efficient_1997,mai_zheng_grace:_2011,michael_boyer_automated_2008}, async-finish programs \cite{raman_scalable_2012} or programs with asynchronous callbacks \cite{petrov_race_2012,raychev_effective_2013,hsiao_race_2014,vechev_oopsla_2015}. Structured parallelism admits more time- and space-efficient data race detection than the general multithreaded programs we support. None of these prior algorithms or implementations have been formally verified, however, so they represent a potentially fruitful area for future work.

\section{Conclusions and Future Work}
\label{conclusion}

We have presented the first verified proofs of correctness of the vector clock and FastTrack race detection algorithms, and a verified implementation of vector clock race detection for a simple multithreaded language. Our verification efforts have revealed an issue in the original paper proof of correctness for FastTrack, and an instance of unnecessary computation in the FastTrack implementation. Our work places dynamic data race detection on a formally-verified foundation for the first time.

We intend to expand our approach to verified race detection along three main lines. First, our approach should generalize easily to verifying more sophisticated algorithms and instrumentation passes, such as an implementation of FastTrack~\cite{fasttrack} or the ThreadSanitizer algorithm used in LLVM~\cite{tsan}. The second and greater challenge will be to apply the same approach on more realistic languages, and ultimately to integrate it into high-assurance compilation frameworks like Vellvm~\cite{vellvm} or CompCert~\cite{compcert}. This would involve taking into account a wider range of program instructions and potential complications, including variable-size accesses and low-level atomics. Thirdly, our proofs thus far assume sequential consistency as the concurrent memory model; an interesting challenge would be to prove that the same instrumentation suffices under the relaxed memory models used in languages like C and Java \TODO{cite memory models?}. By extending the reach of verified race detection, we aim to make it easier to design and justify increasingly sophisticated race detection algorithms and implementations.


\bibliographystyle{abbrvnat}
\bibliography{sources}

% !! The bibliography should be embedded for final submission.

\end{document}
